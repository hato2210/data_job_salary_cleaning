{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd \n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobs(keyword, num_jobs, verbose):\n",
    "    \n",
    "    '''Gathers jobs as a dataframe, scraped from Seek'''\n",
    "    \n",
    "    # Initializing the webdriver.\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument('headless') # Uncomment the left code for browserless scraping.\n",
    "    service = Service(executable_path='/Users/tonyha/Documents/Projects/salary_prediction/chromedriver')\n",
    "    driver = webdriver.Chrome(service = service, options=options)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    keyword = keyword.replace(\" \", \"-\") # Seek requires '-' between the words in the url.\n",
    "    url = \"https://www.seek.com.au/\"+keyword+\"-jobs\"\n",
    "    driver.get(url)\n",
    "    jobs = [] # We store our jobs. It will be a list of dictionaries. \n",
    "\n",
    "    time.sleep(4) # The waiting time (in seconds) between requests. Ensure it is high enough to load pages.\n",
    "\n",
    "    body = driver.find_element(By.XPATH, \"/html/body\") # Get the body of the html so we can use it to scroll the page up.\n",
    "\n",
    "    while len(jobs) < num_jobs:  # If true, should be still looking for new jobs. \n",
    "\n",
    "        # 'job_buttons' contains all the jobs on the page.\n",
    "        job_buttons = driver.find_elements(By.XPATH, \"//*[@id='app']/div/div[3]/div/section/div[2]/div/div/div[1]/div/div/div[1]/div/div/div[1]/div[3]/div\") \n",
    "        \n",
    "        for job_button in job_buttons: # Going through each job on the page.\n",
    "\n",
    "            if len(jobs) >= num_jobs: # If we have collected enough jobs mid-way through the page then stop. \n",
    "                break\n",
    "\n",
    "            # Click the job listing to open its javascript components. Allows us to obtain valuable info under this job. \n",
    "            job_button.find_element(By.XPATH, \".//a[@data-testid='job-list-item-link-overlay']\").click()\n",
    "            time.sleep(4)\n",
    "\n",
    "            # Collect information on the job title, location, and description. These should always be available under each job, and we make \n",
    "            # sure we obtain them by using a while loop. \n",
    "            collected_successfully = False\n",
    "            while not collected_successfully:\n",
    "                try:\n",
    "                    job_title = driver.find_element(By.XPATH, \"//h1[@data-automation='job-detail-title']\").text\n",
    "                    location = driver.find_element(By.XPATH, \"//span[@data-automation='job-detail-location']\").text\n",
    "                    job_description = driver.find_element(By.XPATH, \"//div[@data-automation='jobAdDetails']\").text\n",
    "                    collected_successfully = True\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "\n",
    "            # Collect other useful information. If they are missing from the job listing we assign \"missing\". \n",
    "            try:\n",
    "                company_name = driver.find_element(By.XPATH, \"//span[@data-automation='advertiser-name']\").text\n",
    "            except NoSuchElementException:\n",
    "                company_name = \"missing\"\n",
    "\n",
    "            try:\n",
    "                employment_type = driver.find_element(By.XPATH, \"//span[@data-automation='job-detail-work-type']\").text\n",
    "            except NoSuchElementException:\n",
    "                employment_type = \"missing\"\n",
    "\n",
    "            try:\n",
    "                industry = driver.find_element(By.XPATH, \"//span[@data-automation='job-detail-classifications']\").text\n",
    "            except NoSuchElementException:\n",
    "                industry = \"missing\"\n",
    "\n",
    "            try:\n",
    "                salary = driver.find_element(By.XPATH, \"//span[@data-automation='job-detail-salary']\").text\n",
    "            except NoSuchElementException:\n",
    "                salary = \"missing\"\n",
    "\n",
    "            # Printing for debugging\n",
    "            if verbose:\n",
    "                print(\"Job Title: {}\".format(job_title))\n",
    "                print(\"Company Name: {}\".format(company_name))\n",
    "                print(\"Location: {}\".format(location))\n",
    "                print(\"Work Model: missing\")\n",
    "                print(\"Employment Type: {}\".format(employment_type))\n",
    "                print(\"Industry: {}\".format(industry))\n",
    "                print(\"Company Type: missing\")\n",
    "                print(\"Company Size: missing\")\n",
    "                print(\"Job Description: {}\".format(job_description[:100]))\n",
    "                print(\"Salary: {}\".format(salary))\n",
    "\n",
    "            # Add the job to 'jobs'.\n",
    "            jobs.append({\"Job Title\": job_title,\n",
    "            \"Company Name\": company_name,\n",
    "            \"Location\": location,\n",
    "            \"Work Model\": \"missing\",\n",
    "            \"Employment Type\": employment_type,\n",
    "            \"Industry\": industry,\n",
    "            \"Company Type\": \"missing\",\n",
    "            \"Company Size\": \"missing\",\n",
    "            \"Job Description\" : job_description,\n",
    "            \"Salary\": salary})\n",
    "\n",
    "            # Print progress.\n",
    "            print(\"Progress: {}\".format(\"\" + str(len(jobs)) + \"/\" + str(num_jobs)))\n",
    "\n",
    "            # There is a \"Save this search\" button which may intercept our clicking of jobs. To prevent this we scroll up before clicking on new jobs.\n",
    "            body.send_keys(Keys.PAGE_UP)\n",
    "            time.sleep(1)\n",
    "\n",
    "        # If more jobs need to be collected then go to the next page.\n",
    "        if len(jobs)<num_jobs:\n",
    "            try:\n",
    "                driver.find_element(By.XPATH, \"//a[@aria-label='Next']\").click()\n",
    "                time.sleep(4)\n",
    "            except NoSuchElementException: # This will happen when the current page is the last one.\n",
    "                print(\"Scraping terminated before reaching target number of jobs. Needed {}, got {}.\".format(num_jobs, len(jobs)))\n",
    "                break\n",
    "            \n",
    "    return pd.DataFrame(jobs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the scraping. \n",
    "df = get_jobs(\"data\", 2000, False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"data_seek.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
